{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the PDFs in raw_pdf/\n",
    "# Convert the PDFs to text in raw_text/\n",
    "# Remove the conclusions in processed/\n",
    "# Create a CSV containing all the information in processed/\n",
    "# Use the pigeon app for annotation based on each raw_text that adds the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove spaces from filenames\n",
    "import os\n",
    "\n",
    "folder_path = \"../data/raw_pdf\"\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if \" \" in filename:  # Check if filename contains spaces\n",
    "        new_filename = filename.replace(\" \", \"_\")  # Remove spaces\n",
    "        os.rename(os.path.join(folder_path, filename), os.path.join(folder_path, new_filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all PDF files\n",
    "# import glob\n",
    "# import sys\n",
    "# sys.path.append(\"../\")\n",
    "# from src import TextReport\n",
    "# pdf_files = glob.glob('../data/raw_pdf/*.pdf')\n",
    "\n",
    "# for pdf_file in pdf_files:\n",
    "#     with open(pdf_file, 'rb') as f:\n",
    "#         # convert pdf to text\n",
    "#         pdf_object = TextReport(f, lang=\"fra\")\n",
    "#         pdf_text = pdf_object.pdf_to_text()\n",
    "#         pdf_text = pdf_text.replace(\"\\n\",\" \")\n",
    "#         # dump the text to a file\n",
    "#         new_file_name = pdf_file.replace('raw_pdf', 'raw_text')\n",
    "#         new_file_name = new_file_name.replace('.pdf', '.txt')\n",
    "#         with open(new_file_name, 'w') as final_f:\n",
    "#             final_f.write(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src import TextReport\n",
    "text_files = glob.glob('../data/raw_text/*.txt')\n",
    "\n",
    "all_path = text_files\n",
    "all_file_names = [text_file.split('/')[-1] for text_file in text_files]\n",
    "all_text = []\n",
    "all_conclusion = []\n",
    "\n",
    "for text_file in text_files:\n",
    "    # Load the text file\n",
    "    with open(text_file, \"r\") as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Convert the text to lower case\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Find the position of the word \"conclusion\"\n",
    "    conclusion_pos = text_lower.find(\"conclusion\")\n",
    "\n",
    "    # Remove all text after the word \"conclusion\"\n",
    "    if conclusion_pos != -1:\n",
    "        text_no_conclu = text[:conclusion_pos]\n",
    "    else:\n",
    "        text_no_conclu = text\n",
    "    # Save the modified text back to the file\n",
    "    all_text.append(text_no_conclu)\n",
    "    all_conclusion.append(text[conclusion_pos:])\n",
    "    new_file_name = text_file.replace('raw_text', 'processed')\n",
    "    with open(new_file_name, \"w\") as file:\n",
    "        open(new_file_name, \"w\").write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_conclusions = [i for i in range(len(all_conclusion)) if len(all_conclusion[i]) <= 10]\n",
    "for index in missing_conclusions:\n",
    "    print(f\"Failed detecting conclusions in report index {index} with filename {all_file_names[index]} \")\n",
    "print(\"You should try to check it by hand !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(lst):\n",
    "    \"\"\"\n",
    "    This function finds the duplicates in a list and returns a list of tuples containing the indices of all the duplicates.\n",
    "    \"\"\"\n",
    "    duplicates = {}\n",
    "    for i, item in enumerate(lst):\n",
    "        if item in duplicates:\n",
    "            duplicates[item].append(i)\n",
    "        else:\n",
    "            duplicates[item] = [i]\n",
    "    \n",
    "    result = [(item, indices) for item, indices in duplicates.items() if len(indices) > 1]\n",
    "    return result\n",
    "\n",
    "duplicates = find_duplicates(all_text)\n",
    "print(\"Duplicated reports: \")\n",
    "for item, indices in duplicates:\n",
    "    print(f\"Item is repeated at indices {all_file_names[indices[0]]} and {all_file_names[indices[1]]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous annotations and resume\n",
    "import pigeon as pg\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "processed_df = \"../data/text_dataset.csv\"\n",
    "cols = ['filename', 'diag', 'path', 'text', \"conclusion\"]\n",
    "\n",
    "if os.path.isfile(processed_df):\n",
    "    # If the file exists, load it into a pandas DataFrame\n",
    "    df = pd.read_csv(processed_df)\n",
    "else:\n",
    "    # If the file does not exist, create a new DataFrame with 3 lists and an empty column\n",
    "    data = {\"filename\": all_file_names,\n",
    "            \"diag\": [np.nan for i in range(len(all_file_names))],\n",
    "            \"path\": all_path,\n",
    "            \"text\": all_text,\n",
    "            \"conclusion\": all_conclusion}\n",
    "    df = pd.DataFrame(data, columns=cols)\n",
    "    df.to_csv(processed_df, index=False)\n",
    "\n",
    "df_diag = df[df.diag.notna()]\n",
    "df_no_diag = df[df.diag.isna()]\n",
    "\n",
    "\n",
    "def annotate_text(data):\n",
    "    html = f\"<p>Filename: {data[1]}</p><p>Conlusion: {data[0]}</p><br/><p> Full text: {data[2]}\"\n",
    "    return html\n",
    "\n",
    "merged_list = list(zip(df_no_diag[\"conclusion\"].to_list(), df_no_diag[\"filename\"].to_list(), df_no_diag[\"text\"].to_list()))\n",
    "# Using pigeon to annotate the texts\n",
    "annotations = pg.annotate(\n",
    "    merged_list,\n",
    "    shuffle=False, \n",
    "    include_skip=False,\n",
    "    options=[\"Nemaline Myopathy\",\"Core Myopathy\",\"Centronuclear Myopathy\",\"NON-CM-OTHER\", \"UNCLEAR\"], \n",
    "    display_fn=lambda data: display(HTML(annotate_text(data))),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Current Annotations\n",
    "for index, value in enumerate(annotations):\n",
    "    df_no_diag.iloc[index, 1] = value[1]\n",
    "df_final = pd.concat([df_diag, df_no_diag])\n",
    "df_final.to_csv(processed_df, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(processed_df)\n",
    "df[\"diag\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import deepl \n",
    "# import pandas as pd\n",
    "# import deepl\n",
    "# from dotenv import load_dotenv\n",
    "# import os\n",
    "# load_dotenv() \n",
    "\n",
    "# translator = deepl.Translator(os.getenv(\"DEEPL_KEY\"))\n",
    "# df_final[\"translated_text\"] = df_final[\"text\"].apply(lambda x: translator.translate_text(x, target_lang=\"EN-US\").text)\n",
    "# df_final.to_csv('../data/text_dataset_translate.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
